{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Final note.ipynb","provenance":[],"collapsed_sections":["gZJznJRC8srI","6z1kmYWHNsK4","L6v_HGOd8y1N","5zNjY6fkN_wP","W3yH8Gsu_kA7","Y-kI3TzTnA9Z","xJTtHacQdHU3"],"toc_visible":true,"authorship_tag":"ABX9TyO2h/x7la1GFibY6ib9c5Nc"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"gZJznJRC8srI"},"source":["# **Import libraries**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-45eEvdJ8nOr","executionInfo":{"status":"ok","timestamp":1614546498086,"user_tz":-60,"elapsed":5151,"user":{"displayName":"tom vesoul","photoUrl":"","userId":"10135283741654588830"}},"outputId":"733e62d3-dd3b-4d60-c9fc-7f591465d0f6"},"source":["!pip install mido"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Collecting mido\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/20/0a/81beb587b1ae832ea6a1901dc7c6faa380e8dd154e0a862f0a9f3d2afab9/mido-1.2.9-py2.py3-none-any.whl (52kB)\n","\r\u001b[K     |██████▎                         | 10kB 19.0MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 20kB 16.7MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 30kB 14.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 40kB 12.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 51kB 8.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 61kB 4.8MB/s \n","\u001b[?25hInstalling collected packages: mido\n","Successfully installed mido-1.2.9\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"cTbDl8PI8fYg","executionInfo":{"status":"ok","timestamp":1614546501577,"user_tz":-60,"elapsed":8639,"user":{"displayName":"tom vesoul","photoUrl":"","userId":"10135283741654588830"}}},"source":["import mido \n","import matplotlib.pyplot as plt \n","import numpy as np \n","import os \n","import random\n","import pandas as pd\n","\n","from mido import MidiFile, MidiTrack, Message\n","\n","from sklearn import model_selection\n","\n","import torch \n","import torch.nn as nn\n","import torchvision\n","import torchvision.transforms as transforms"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C_IWJPBcOPBF","executionInfo":{"status":"ok","timestamp":1614546501578,"user_tz":-60,"elapsed":8635,"user":{"displayName":"tom vesoul","photoUrl":"","userId":"10135283741654588830"}},"outputId":"499a7663-e0dd-4e30-9faf-0203b598103d"},"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","device"],"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["device(type='cuda')"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"markdown","metadata":{"id":"6z1kmYWHNsK4"},"source":["# **Hyperparameters**"]},{"cell_type":"code","metadata":{"id":"H39jJFdVNvUu","executionInfo":{"status":"ok","timestamp":1614546724633,"user_tz":-60,"elapsed":464,"user":{"displayName":"tom vesoul","photoUrl":"","userId":"10135283741654588830"}}},"source":["num_epochs = 100\n","batch_size = 1024\n","\n","sequence_length = 16\n","embedding_dim = 128\n","\n","hidden_size = 512\n","num_layers = 3\n","num_classes = 128\n","\n","learning_rate = 0.05"],"execution_count":11,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"L6v_HGOd8y1N"},"source":["# **Load data**"]},{"cell_type":"code","metadata":{"id":"xj0LRySn8jq3","executionInfo":{"status":"ok","timestamp":1614546759824,"user_tz":-60,"elapsed":624,"user":{"displayName":"tom vesoul","photoUrl":"","userId":"10135283741654588830"}}},"source":["df_train = pd.read_csv('train_note.csv', header=None)\n","df_val = pd.read_csv('val_note.csv', header=None)\n","\n","array_train = df_train.values.astype(int)\n","array_val = df_val.values.astype(int)"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"BLWHch9k_gI2","executionInfo":{"status":"ok","timestamp":1614546759825,"user_tz":-60,"elapsed":477,"user":{"displayName":"tom vesoul","photoUrl":"","userId":"10135283741654588830"}}},"source":["train_loader = torch.utils.data.DataLoader(dataset=array_train,\n","                                           batch_size=batch_size, \n","                                           shuffle=True)\n","\n","val_loader = torch.utils.data.DataLoader(dataset=array_val,\n","                                           batch_size=batch_size, \n","                                           shuffle=True)"],"execution_count":14,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5zNjY6fkN_wP"},"source":["# **Models**"]},{"cell_type":"markdown","metadata":{"id":"-HXNZTvaTAPM"},"source":["## Classification approach"]},{"cell_type":"code","metadata":{"id":"iIiAEO88PNcD","executionInfo":{"status":"ok","timestamp":1614547311992,"user_tz":-60,"elapsed":471,"user":{"displayName":"tom vesoul","photoUrl":"","userId":"10135283741654588830"}}},"source":["# RNN architecture\n","class RNN(nn.Module):\n","    def __init__(self, num_classes, embedding_dim,  hidden_size, num_layers, drop_prob=0.):\n","        super(RNN, self).__init__()\n","\n","        self.embedding = nn.Embedding(num_classes, embedding_dim)\n","\n","        self.hidden_size = hidden_size\n","        self.num_layers = num_layers\n","        self.lstm = nn.RNN(embedding_dim, hidden_size, num_layers, dropout=drop_prob, batch_first=True)\n","        self.fc_1 = nn.Linear(hidden_size, hidden_size)\n","        self.fc_2 = nn.Linear(hidden_size, num_classes)\n","\n","        self.relu = nn.ReLU()\n","    \n","    def forward(self, x):\n","\n","        # Embedding layer\n","        x = self.embedding(x) # Output shape (batch, sequence_length, embedding_dim)\n","\n","        # Set initial hidden and cell states \n","        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device) \n","        \n","        # Forward propagate LSTM\n","        out, hidden = self.lstm(x, h0)  # out: tensor of shape (batch_size, seq_length, hidden_size)\n","        out = out[:, -1, :] # Hidden state of the last element of the sequence \n","        \n","        #FC\n","        out = self.fc_1(out)\n","        out = self.relu(out)\n","        out = self.fc_2(out)\n","        return out"],"execution_count":27,"outputs":[]},{"cell_type":"code","metadata":{"id":"3YDQJl-NePuh","executionInfo":{"status":"ok","timestamp":1614547312281,"user_tz":-60,"elapsed":596,"user":{"displayName":"tom vesoul","photoUrl":"","userId":"10135283741654588830"}}},"source":["# GRU architecture\n","class GRU(nn.Module):\n","    def __init__(self, num_classes, embedding_dim,  hidden_size, num_layers, drop_prob=0.):\n","        super(GRU, self).__init__()\n","\n","        self.embedding = nn.Embedding(num_classes, embedding_dim)\n","\n","        self.hidden_size = hidden_size\n","        self.num_layers = num_layers\n","        self.lstm = nn.GRU(embedding_dim, hidden_size, num_layers, dropout=drop_prob, batch_first=True)\n","        self.fc_1 = nn.Linear(hidden_size, hidden_size)\n","        self.fc_2 = nn.Linear(hidden_size, num_classes)\n","\n","        self.relu = nn.ReLU()\n","    \n","    def forward(self, x):\n","\n","        # Embedding layer\n","        x = self.embedding(x) # Output shape (batch, sequence_length, embedding_dim)\n","\n","        # Set initial hidden and cell states \n","        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device) \n","        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n","        \n","        # Forward propagate LSTM\n","        out, hidden = self.lstm(x, h0)  # out: tensor of shape (batch_size, seq_length, hidden_size)\n","        out = out[:, -1, :] # Hidden state of the last element of the sequence \n","        #FC\n","        out = self.fc_1(out)\n","        out = self.relu(out)\n","        out = self.fc_2(out)\n","        return out"],"execution_count":28,"outputs":[]},{"cell_type":"code","metadata":{"id":"_LCBDOXteaoq","executionInfo":{"status":"ok","timestamp":1614547312281,"user_tz":-60,"elapsed":448,"user":{"displayName":"tom vesoul","photoUrl":"","userId":"10135283741654588830"}}},"source":["# LSTM architecture \n","class LSTM(nn.Module):\n","    def __init__(self, num_classes, embedding_dim,  hidden_size, num_layers, drop_prob=0.):\n","        super(LSTM, self).__init__()\n","\n","        self.embedding = nn.Embedding(num_classes, embedding_dim)\n","\n","        self.hidden_size = hidden_size\n","        self.num_layers = num_layers\n","        self.lstm = nn.LSTM(embedding_dim, hidden_size, num_layers, dropout=drop_prob, batch_first=True)\n","        self.fc_1 = nn.Linear(hidden_size, hidden_size)\n","        self.fc_2 = nn.Linear(hidden_size, num_classes)\n","\n","        self.relu = nn.ReLU()\n","    \n","    def forward(self, x):\n","\n","        # Embedding layer\n","        x = self.embedding(x) # Output shape (batch, sequence_length, embedding_dim)\n","\n","        # Set initial hidden and cell states \n","        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device) \n","        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n","        \n","        # Forward propagate LSTM\n","        out, hidden = self.lstm(x, (h0, c0))  # out: tensor of shape (batch_size, seq_length, hidden_size)\n","        out = out[:, -1, :] # Hidden state of the last element of the sequence \n","        \n","        #FC\n","        out = self.fc_1(out)\n","        out = self.relu(out)\n","        out = self.fc_2(out)\n","        return out"],"execution_count":29,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"W3yH8Gsu_kA7"},"source":["# **Training**"]},{"cell_type":"markdown","metadata":{"id":"HcOcQvB3fE-f"},"source":["## Accuracy "]},{"cell_type":"code","metadata":{"id":"nBIn4DTjfOnw","executionInfo":{"status":"ok","timestamp":1614546764104,"user_tz":-60,"elapsed":446,"user":{"displayName":"tom vesoul","photoUrl":"","userId":"10135283741654588830"}}},"source":["def validate_model(model, loader):\n","    model.eval()\n","    with torch.no_grad():\n","        correct = 0\n","        total = 0\n","        for batch in loader:\n","            sequence = batch[:,:16].to(device)\n","            target = batch[:,16].to(device)\n","            outputs = model(sequence)\n","\n","            _, predicted = torch.max(outputs.data, 1)\n","            total += target.size(0)\n","            correct += (predicted == target).sum().item()\n","        accuracy = 100 * correct / total\n","\n","    return (accuracy)"],"execution_count":18,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0H3umlWRfPNJ"},"source":["## Training loop"]},{"cell_type":"code","metadata":{"id":"ArkqUsBtmsQT","executionInfo":{"status":"ok","timestamp":1614546764833,"user_tz":-60,"elapsed":480,"user":{"displayName":"tom vesoul","photoUrl":"","userId":"10135283741654588830"}}},"source":["def train_model(model, optimizer, train_loader, val_loader, num_epochs, lr_scheduler=None, display_loss=False):\n","  criterion = nn.CrossEntropyLoss()\n","\n","  best_val_accuracy = 0\n","  best_epoch = 0\n","\n","  for epoch in range(num_epochs):\n","\n","    model.train()\n","\n","    #### UPDATE LEARNING RATE #### \n","    if lr_scheduler == 'multi_steps':\n","        if epoch in [int(num_epochs * 0.5)]:\n","            for param_group in optimizer.param_groups:\n","                param_group['lr'] *= 0.1\n","\n","    for i, batch in enumerate(train_loader):\n","      sequence = batch[:,:16].to(device)\n","      target = batch[:,16].to(device)\n","\n","      optimizer.zero_grad()\n","      outputs = model(sequence)\n","      loss = criterion(outputs, target)\n","      loss.backward()\n","      optimizer.step()\n","\n","      if i % 300 == 0 and display_loss:\n","        print(f'Epoch : {epoch}, Step: {i}, Loss: {round(loss.item(), 2)}')\n","\n","    # Train accuracy \n","    train_accuracy = round(validate_model(model, train_loader), 2)\n","\n","    # Val accuracy\n","    val_accuracy = round(validate_model(model, val_loader), 2)\n","    if val_accuracy > best_val_accuracy:\n","      best_val_accuracy = val_accuracy\n","      best_epoch = epoch\n","\n","    print('################')\n","    print(f'Epoch : {epoch}, Train accuracy : {train_accuracy} %, Val accuracy : {val_accuracy} %')\n","    print(f'Best val accuracy at epoch {best_epoch}: {best_val_accuracy} %')"],"execution_count":19,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Y-kI3TzTnA9Z"},"source":["# **Experiments**"]},{"cell_type":"markdown","metadata":{"id":"XWvEjPTYgQkF"},"source":["## RNN architecture"]},{"cell_type":"code","metadata":{"id":"0YBWbF78fSLT"},"source":["model = RNN(num_classes, embedding_dim=128, hidden_size=512, num_layers=3).to(device)\n","optimizer = torch.optim.SGD(model.parameters(), lr=0.05, nesterov=True, momentum=0.9)\n","\n","train_model(model, optimizer, train_loader, val_loader, num_epochs=100, lr_scheduler='multi_steps')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qFSbvh4fgTXV"},"source":["## GRU architecture"]},{"cell_type":"code","metadata":{"id":"zo9SeWJHfSpM"},"source":["model = GRU(num_classes, embedding_dim=128, hidden_size=512, num_layers=3).to(device)\n","optimizer = torch.optim.SGD(model.parameters(), lr=0.05, nesterov=True, momentum=0.9)\n","\n","train_model(model, optimizer, train_loader, val_loader, num_epochs=100, lr_scheduler='multi_steps')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gavLcNDRgWnQ"},"source":["## LSTM architecture"]},{"cell_type":"code","metadata":{"id":"IXir1LNffPKj"},"source":["model = LSTM(num_classes, embedding_dim=128, hidden_size=512, num_layers=3).to(device)\n","optimizer = torch.optim.SGD(model.parameters(), lr=0.05, nesterov=True, momentum=0.9)\n","\n","train_model(model, optimizer, train_loader, val_loader, num_epochs=100, lr_scheduler='multi_steps')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xJTtHacQdHU3"},"source":["# **Test predictions**"]},{"cell_type":"markdown","metadata":{"id":"tPe8tBGFd_WX"},"source":["## Mido utils"]},{"cell_type":"code","metadata":{"id":"li_vB3yOdm6p","executionInfo":{"status":"ok","timestamp":1614547104717,"user_tz":-60,"elapsed":487,"user":{"displayName":"tom vesoul","photoUrl":"","userId":"10135283741654588830"}}},"source":["def notes_to_track(notes):\n","    track = MidiTrack()\n","    for note in notes:\n","        #track.append(Message('note_on', channel=0, note=note, velocity=64, time=0))\n","        #track.append(Message('note_on', channel=0, note=note, velocity=0, time=240))\n","        \n","        # Longueur par défault 480: 4 mesures. Valeur du time : espace entre chaque note si <480 chevauchement\n","        track.append(Message('note_on', channel=0, note=note, velocity=64, time=240))\n","    return(track)\n","    "],"execution_count":23,"outputs":[]},{"cell_type":"code","metadata":{"id":"dUh7p2jveSfs","executionInfo":{"status":"ok","timestamp":1614547105020,"user_tz":-60,"elapsed":467,"user":{"displayName":"tom vesoul","photoUrl":"","userId":"10135283741654588830"}}},"source":["def save_track(track, path):\n","    mid = MidiFile()\n","    mid.tracks.append(track)\n","    mid.save(path)"],"execution_count":24,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"n8Kv-8hXg8f5"},"source":["## Generation"]},{"cell_type":"code","metadata":{"id":"0mhfgO-Ao97p","executionInfo":{"status":"ok","timestamp":1614547206427,"user_tz":-60,"elapsed":2345,"user":{"displayName":"tom vesoul","photoUrl":"","userId":"10135283741654588830"}}},"source":["# Random sampling\n","n_predictions = 1000\n","temp = 2 # temperature parameter\n","list_notes = [64]\n","\n","for i in range(n_predictions):\n","  list_input = list_notes[-16:]\n","  input = torch.reshape(torch.tensor(list_input),(1,-1)).to(device)\n","  pred = model(input)\n","\n","  array_proba = torch.softmax(pred / temp, 1).detach().cpu().numpy()[0]\n","  note = np.random.choice(range(num_classes), p=array_proba)\n","  list_notes.append(note)\n","\n","midi = notes_to_track(list_notes)\n","save_track(midi, 'file.mid')"],"execution_count":26,"outputs":[]}]}