{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Cond all final.ipynb","provenance":[],"collapsed_sections":["gZJznJRC8srI","6z1kmYWHNsK4","L6v_HGOd8y1N","5zNjY6fkN_wP","W3yH8Gsu_kA7","Y-kI3TzTnA9Z","xJTtHacQdHU3"],"toc_visible":true,"authorship_tag":"ABX9TyOQRjp9km40hY4tlzEaTGKv"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"gZJznJRC8srI"},"source":["# **Import libraries**"]},{"cell_type":"code","metadata":{"id":"-45eEvdJ8nOr","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1614550148792,"user_tz":-60,"elapsed":2535,"user":{"displayName":"tom vesoul","photoUrl":"","userId":"10135283741654588830"}},"outputId":"4304820c-3a07-41ad-f179-2f98a81fdddf"},"source":["!pip install mido"],"execution_count":24,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: mido in /usr/local/lib/python3.7/dist-packages (1.2.9)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"cTbDl8PI8fYg","executionInfo":{"status":"ok","timestamp":1614550148793,"user_tz":-60,"elapsed":2329,"user":{"displayName":"tom vesoul","photoUrl":"","userId":"10135283741654588830"}}},"source":["import mido # easy to use python MIDI library\n","import matplotlib.pyplot as plt # plotting\n","import numpy as np # linear algebra\n","import os # accessing directory structure\n","import random\n","import pandas as pd\n","\n","from mido import MidiFile, MidiTrack, Message\n","\n","from sklearn import model_selection\n","\n","import torch \n","import torch.nn as nn\n","import torchvision\n","import torchvision.transforms as transforms\n","import torch.nn.functional as F"],"execution_count":25,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C_IWJPBcOPBF","executionInfo":{"status":"ok","timestamp":1614550148793,"user_tz":-60,"elapsed":2225,"user":{"displayName":"tom vesoul","photoUrl":"","userId":"10135283741654588830"}},"outputId":"0044e631-76bd-40b5-f668-bff4f22b0013"},"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","device"],"execution_count":26,"outputs":[{"output_type":"execute_result","data":{"text/plain":["device(type='cuda')"]},"metadata":{"tags":[]},"execution_count":26}]},{"cell_type":"markdown","metadata":{"id":"6z1kmYWHNsK4"},"source":["# **Hyperparameters**"]},{"cell_type":"code","metadata":{"id":"H39jJFdVNvUu","executionInfo":{"status":"ok","timestamp":1614550148793,"user_tz":-60,"elapsed":596,"user":{"displayName":"tom vesoul","photoUrl":"","userId":"10135283741654588830"}}},"source":["num_epochs = 100\n","batch_size = 1024\n","\n","sequence_length = 16\n","embedding_dim_note = 128\n","embedding_dim_duree = 128\n","embedding_dim_time = 128\n","embedding_dim_veloc = 128\n","\n","hidden_size = 512\n","num_layers = 3\n","num_classes_note = 128\n","num_classes_duree = 32\n","num_classes_time = 18\n","\n","learning_rate = 0.05"],"execution_count":27,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"L6v_HGOd8y1N"},"source":["# **Load data**"]},{"cell_type":"code","metadata":{"id":"xj0LRySn8jq3","executionInfo":{"status":"ok","timestamp":1614550150678,"user_tz":-60,"elapsed":1590,"user":{"displayName":"tom vesoul","photoUrl":"","userId":"10135283741654588830"}}},"source":["df_train = pd.read_csv('train_all.csv', header=None)\n","df_val = pd.read_csv('val_all.csv', header=None)\n","\n","array_train = df_train.values\n","array_val = df_val.values"],"execution_count":28,"outputs":[]},{"cell_type":"code","metadata":{"id":"BLWHch9k_gI2","executionInfo":{"status":"ok","timestamp":1614550150678,"user_tz":-60,"elapsed":1165,"user":{"displayName":"tom vesoul","photoUrl":"","userId":"10135283741654588830"}}},"source":["train_loader = torch.utils.data.DataLoader(dataset=array_train,\n","                                           batch_size=batch_size, \n","                                           shuffle=True)\n","\n","val_loader = torch.utils.data.DataLoader(dataset=array_val,\n","                                           batch_size=batch_size, \n","                                           shuffle=True)"],"execution_count":29,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5zNjY6fkN_wP"},"source":["# **Models**"]},{"cell_type":"markdown","metadata":{"id":"8-vWA9zWrZ6d"},"source":["## RNN"]},{"cell_type":"code","metadata":{"id":"iIiAEO88PNcD","executionInfo":{"status":"ok","timestamp":1614550242260,"user_tz":-60,"elapsed":526,"user":{"displayName":"tom vesoul","photoUrl":"","userId":"10135283741654588830"}}},"source":["# Recurrent neural network (many-to-one)\n","class RNN(nn.Module):\n","  def __init__(self, num_classes_note, num_classes_duree, num_classes_time, embedding_dim_note, embedding_dim_duree, embedding_dim_time, embedding_dim_veloc, hidden_size, num_layers, drop_prob=0., drop_fc=0.):\n","    super(RNN, self).__init__()\n","\n","    self.embedding_note = nn.Embedding(num_classes_note, embedding_dim_note)\n","    self.embedding_duree = nn.Embedding(num_classes_duree, embedding_dim_duree)\n","    self.embedding_time = nn.Embedding(num_classes_time, embedding_dim_time)\n","    self.fc_emb_veloc = nn.Linear(1, embedding_dim_veloc)\n","\n","    self.embedding_note_cond = nn.Embedding(num_classes_note, embedding_dim_note)\n","    self.embedding_duree_cond = nn.Embedding(num_classes_duree, embedding_dim_duree)\n","    self.embedding_time_cond = nn.Embedding(num_classes_time, embedding_dim_time)\n","\n","    self.hidden_size = hidden_size\n","    self.num_layers = num_layers\n","    self.rnn = nn.RNN(embedding_dim_note + embedding_dim_duree + embedding_dim_time + embedding_dim_veloc, hidden_size, num_layers, dropout=drop_prob, batch_first=True)\n","    \n","    self.fc_note_1 = nn.Linear(hidden_size, hidden_size)\n","    self.fc_note_2 = nn.Linear(hidden_size, num_classes_note)\n","\n","    self.fc_duree_1 = nn.Linear(hidden_size + embedding_dim_note, hidden_size)\n","    self.fc_duree_2 = nn.Linear(hidden_size, num_classes_duree)\n","\n","    self.fc_time_1 = nn.Linear(hidden_size + embedding_dim_note + embedding_dim_duree, hidden_size)\n","    self.fc_time_2 = nn.Linear(hidden_size, num_classes_time)\n","\n","    self.fc_veloc_1 = nn.Linear(hidden_size + embedding_dim_note + embedding_dim_duree + embedding_dim_time, hidden_size)\n","    self.fc_veloc_2 = nn.Linear(hidden_size, 1)\n","\n","    self.relu = nn.ReLU()\n","    self.dropout = nn.Dropout(p=drop_fc)\n","\n","  def foward_RNN(self, inputs):\n","    notes, duree, time, veloc = inputs\n","\n","    # Embedding layer\n","    embeddings_note = self.embedding_note(notes) # Output shape (batch, sequence_length, embedding_dim)\n","    embeddings_duree = self.embedding_duree(duree)\n","    embeddings_time = self.embedding_time(time)\n","\n","    emeddings_veloc = self.fc_emb_veloc(veloc)\n","\n","    x = torch.cat((embeddings_note, embeddings_duree, embeddings_time, emeddings_veloc), dim=2)\n","\n","    # Set initial hidden and cell states \n","    h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device) \n","    \n","    # Forward propagate LSTM\n","    out, hidden = self.rnn(x, h0)  # out: tensor of shape (batch_size, seq_length, hidden_size)\n","    out = out[:, -1, :] # Hidden state of the last element of the sequence (equivalent to hidden[0])\n","\n","    return out\n","\n","  def classif_note(self, out):\n","    note = self.fc_note_1(out)\n","    note = self.dropout(note)\n","    note = self.relu(note)\n","    note = self.fc_note_2(note)\n","    return note\n","\n","  def classif_duree(self, out, note_target):\n","\n","    embeddings_note_target = self.embedding_note_cond(note_target)\n","\n","    duree = torch.cat((out, embeddings_note_target), dim=1)\n","    duree = self.fc_duree_1(duree)\n","    duree = self.dropout(duree)\n","    duree = self.relu(duree)\n","    duree = self.fc_duree_2(duree)\n","    return duree\n","\n","  def classif_time(self, out, note_target, duree_target):\n","\n","    embeddings_note_target = self.embedding_note_cond(note_target)\n","    embeddings_duree_target = self.embedding_duree_cond(duree_target)\n","\n","    time = torch.cat((out, embeddings_note_target, embeddings_duree_target), dim=1)\n","    time = self.fc_time_1(time)\n","    time = self.dropout(time)\n","    time = self.relu(time)\n","    time = self.fc_time_2(time)\n","    return time\n","\n","  def reg_veloc(self, out, note_target, duree_target, time_target):\n","\n","    embeddings_note_target = self.embedding_note_cond(note_target)\n","    embeddings_duree_target = self.embedding_duree_cond(duree_target)\n","    embeddings_time_target = self.embedding_time_cond(time_target)\n","\n","    veloc = torch.cat((out, embeddings_note_target, embeddings_duree_target, embeddings_time_target), dim=1)\n","    veloc = self.fc_veloc_1(veloc)\n","    veloc = self.dropout(veloc)\n","    veloc = self.relu(veloc)\n","    veloc = self.fc_veloc_2(veloc)\n","    return veloc\n","\n","\n","  def forward(self, inputs, targets):\n","    out = self.foward_RNN(inputs)\n","\n","    note_target, duree_target, time_target = targets\n","\n","    note = self.classif_note(out)\n","    duree = self.classif_duree(out, note_target)\n","    time = self.classif_time(out, note_target, duree_target)\n","    veloc = self.reg_veloc(out, note_target, duree_target, time_target)\n","\n","    return note, duree, time, veloc"],"execution_count":38,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5h1SCGjarf7R"},"source":["## GRU"]},{"cell_type":"code","metadata":{"id":"eSjLbB3Irgls","executionInfo":{"status":"ok","timestamp":1614550242690,"user_tz":-60,"elapsed":530,"user":{"displayName":"tom vesoul","photoUrl":"","userId":"10135283741654588830"}}},"source":["# Recurrent neural network (many-to-one)\n","class GRU(nn.Module):\n","  def __init__(self, num_classes_note, num_classes_duree, num_classes_time, embedding_dim_note, embedding_dim_duree, embedding_dim_time, embedding_dim_veloc, hidden_size, num_layers, drop_prob=0., drop_fc=0.):\n","    super(GRU, self).__init__()\n","\n","    self.embedding_note = nn.Embedding(num_classes_note, embedding_dim_note)\n","    self.embedding_duree = nn.Embedding(num_classes_duree, embedding_dim_duree)\n","    self.embedding_time = nn.Embedding(num_classes_time, embedding_dim_time)\n","    self.fc_emb_veloc = nn.Linear(1, embedding_dim_veloc)\n","\n","    self.embedding_note_cond = nn.Embedding(num_classes_note, embedding_dim_note)\n","    self.embedding_duree_cond = nn.Embedding(num_classes_duree, embedding_dim_duree)\n","    self.embedding_time_cond = nn.Embedding(num_classes_time, embedding_dim_time)\n","\n","    self.hidden_size = hidden_size\n","    self.num_layers = num_layers\n","    self.rnn = nn.GRU(embedding_dim_note + embedding_dim_duree + embedding_dim_time + embedding_dim_veloc, hidden_size, num_layers, dropout=drop_prob, batch_first=True)\n","    \n","    self.fc_note_1 = nn.Linear(hidden_size, hidden_size)\n","    self.fc_note_2 = nn.Linear(hidden_size, num_classes_note)\n","\n","    self.fc_duree_1 = nn.Linear(hidden_size + embedding_dim_note, hidden_size)\n","    self.fc_duree_2 = nn.Linear(hidden_size, num_classes_duree)\n","\n","    self.fc_time_1 = nn.Linear(hidden_size + embedding_dim_note + embedding_dim_duree, hidden_size)\n","    self.fc_time_2 = nn.Linear(hidden_size, num_classes_time)\n","\n","    self.fc_veloc_1 = nn.Linear(hidden_size + embedding_dim_note + embedding_dim_duree + embedding_dim_time, hidden_size)\n","    self.fc_veloc_2 = nn.Linear(hidden_size, 1)\n","\n","    self.relu = nn.ReLU()\n","    self.dropout = nn.Dropout(p=drop_fc)\n","\n","  def foward_RNN(self, inputs):\n","    notes, duree, time, veloc = inputs\n","\n","    # Embedding layer\n","    embeddings_note = self.embedding_note(notes) # Output shape (batch, sequence_length, embedding_dim)\n","    embeddings_duree = self.embedding_duree(duree)\n","    embeddings_time = self.embedding_time(time)\n","\n","    emeddings_veloc = self.fc_emb_veloc(veloc)\n","\n","    x = torch.cat((embeddings_note, embeddings_duree, embeddings_time, emeddings_veloc), dim=2)\n","\n","    # Set initial hidden and cell states \n","    h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device) \n","    \n","    # Forward propagate LSTM\n","    out, hidden = self.rnn(x, h0)  # out: tensor of shape (batch_size, seq_length, hidden_size)\n","    out = out[:, -1, :] # Hidden state of the last element of the sequence (equivalent to hidden[0])\n","\n","    return out\n","\n","  def classif_note(self, out):\n","    note = self.fc_note_1(out)\n","    note = self.dropout(note)\n","    note = self.relu(note)\n","    note = self.fc_note_2(note)\n","    return note\n","\n","  def classif_duree(self, out, note_target):\n","\n","    embeddings_note_target = self.embedding_note_cond(note_target)\n","\n","    duree = torch.cat((out, embeddings_note_target), dim=1)\n","    duree = self.fc_duree_1(duree)\n","    duree = self.dropout(duree)\n","    duree = self.relu(duree)\n","    duree = self.fc_duree_2(duree)\n","    return duree\n","\n","  def classif_time(self, out, note_target, duree_target):\n","\n","    embeddings_note_target = self.embedding_note_cond(note_target)\n","    embeddings_duree_target = self.embedding_duree_cond(duree_target)\n","\n","    time = torch.cat((out, embeddings_note_target, embeddings_duree_target), dim=1)\n","    time = self.fc_time_1(time)\n","    time = self.dropout(time)\n","    time = self.relu(time)\n","    time = self.fc_time_2(time)\n","    return time\n","\n","  def reg_veloc(self, out, note_target, duree_target, time_target):\n","\n","    embeddings_note_target = self.embedding_note_cond(note_target)\n","    embeddings_duree_target = self.embedding_duree_cond(duree_target)\n","    embeddings_time_target = self.embedding_time_cond(time_target)\n","\n","    veloc = torch.cat((out, embeddings_note_target, embeddings_duree_target, embeddings_time_target), dim=1)\n","    veloc = self.fc_veloc_1(veloc)\n","    veloc = self.dropout(veloc)\n","    veloc = self.relu(veloc)\n","    veloc = self.fc_veloc_2(veloc)\n","    return veloc\n","\n","\n","  def forward(self, inputs, targets):\n","    out = self.foward_RNN(inputs)\n","\n","    note_target, duree_target, time_target = targets\n","\n","    note = self.classif_note(out)\n","    duree = self.classif_duree(out, note_target)\n","    time = self.classif_time(out, note_target, duree_target)\n","    veloc = self.reg_veloc(out, note_target, duree_target, time_target)\n","\n","    return note, duree, time, veloc"],"execution_count":39,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cTSo41airfEJ"},"source":["## LSTM"]},{"cell_type":"code","metadata":{"id":"PX8vcszkrhwC","executionInfo":{"status":"ok","timestamp":1614550245114,"user_tz":-60,"elapsed":455,"user":{"displayName":"tom vesoul","photoUrl":"","userId":"10135283741654588830"}}},"source":["# Recurrent neural network (many-to-one)\n","class LSTM(nn.Module):\n","  def __init__(self, num_classes_note, num_classes_duree, num_classes_time, embedding_dim_note, embedding_dim_duree, embedding_dim_time, embedding_dim_veloc, hidden_size, num_layers, drop_prob=0., drop_fc=0.):\n","    super(LSTM, self).__init__()\n","\n","    self.embedding_note = nn.Embedding(num_classes_note, embedding_dim_note)\n","    self.embedding_duree = nn.Embedding(num_classes_duree, embedding_dim_duree)\n","    self.embedding_time = nn.Embedding(num_classes_time, embedding_dim_time)\n","    self.fc_emb_veloc = nn.Linear(1, embedding_dim_veloc)\n","\n","    self.embedding_note_cond = nn.Embedding(num_classes_note, embedding_dim_note)\n","    self.embedding_duree_cond = nn.Embedding(num_classes_duree, embedding_dim_duree)\n","    self.embedding_time_cond = nn.Embedding(num_classes_time, embedding_dim_time)\n","\n","    self.hidden_size = hidden_size\n","    self.num_layers = num_layers\n","    self.rnn = nn.LSTM(embedding_dim_note + embedding_dim_duree + embedding_dim_time + embedding_dim_veloc, hidden_size, num_layers, dropout=drop_prob, batch_first=True)\n","    \n","    self.fc_note_1 = nn.Linear(hidden_size, hidden_size)\n","    self.fc_note_2 = nn.Linear(hidden_size, num_classes_note)\n","\n","    self.fc_duree_1 = nn.Linear(hidden_size + embedding_dim_note, hidden_size)\n","    self.fc_duree_2 = nn.Linear(hidden_size, num_classes_duree)\n","\n","    self.fc_time_1 = nn.Linear(hidden_size + embedding_dim_note + embedding_dim_duree, hidden_size)\n","    self.fc_time_2 = nn.Linear(hidden_size, num_classes_time)\n","\n","    self.fc_veloc_1 = nn.Linear(hidden_size + embedding_dim_note + embedding_dim_duree + embedding_dim_time, hidden_size)\n","    self.fc_veloc_2 = nn.Linear(hidden_size, 1)\n","\n","\n","    self.relu = nn.ReLU()\n","    self.dropout = nn.Dropout(p=drop_fc)\n","\n","  def foward_RNN(self, inputs):\n","    notes, duree, time, veloc = inputs\n","\n","    # Embedding layer\n","    embeddings_note = self.embedding_note(notes) # Output shape (batch, sequence_length, embedding_dim)\n","    embeddings_duree = self.embedding_duree(duree)\n","    embeddings_time = self.embedding_time(time)\n","\n","    emeddings_veloc = self.fc_emb_veloc(veloc)\n","    #emeddings_veloc =  torch.unsqueeze(emeddings_veloc, 2)\n","\n","    x = torch.cat((embeddings_note, embeddings_duree, embeddings_time, emeddings_veloc), dim=2)\n","\n","    # Set initial hidden and cell states \n","    h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device) \n","    c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device) \n","    \n","    # Forward propagate LSTM\n","    out, hidden = self.rnn(x, (h0, c0))  # out: tensor of shape (batch_size, seq_length, hidden_size)\n","    out = out[:, -1, :] # Hidden state of the last element of the sequence (equivalent to hidden[0])\n","\n","    return out\n","\n","  def classif_note(self, out):\n","    note = self.fc_note_1(out)\n","    note = self.dropout(note)\n","    note = self.relu(note)\n","    note = self.fc_note_2(note)\n","    return note\n","\n","  def classif_duree(self, out, note_target):\n","\n","    embeddings_note_target = self.embedding_note_cond(note_target)\n","\n","    duree = torch.cat((out, embeddings_note_target), dim=1)\n","    duree = self.fc_duree_1(duree)\n","    duree = self.dropout(duree)\n","    duree = self.relu(duree)\n","    duree = self.fc_duree_2(duree)\n","    return duree\n","\n","  def classif_time(self, out, note_target, duree_target):\n","\n","    embeddings_note_target = self.embedding_note_cond(note_target)\n","    embeddings_duree_target = self.embedding_duree_cond(duree_target)\n","\n","    time = torch.cat((out, embeddings_note_target, embeddings_duree_target), dim=1)\n","    time = self.fc_time_1(time)\n","    time = self.dropout(time)\n","    time = self.relu(time)\n","    time = self.fc_time_2(time)\n","    return time\n","\n","  def reg_veloc(self, out, note_target, duree_target, time_target):\n","\n","    embeddings_note_target = self.embedding_note_cond(note_target)\n","    embeddings_duree_target = self.embedding_duree_cond(duree_target)\n","    embeddings_time_target = self.embedding_time_cond(time_target)\n","\n","    veloc = torch.cat((out, embeddings_note_target, embeddings_duree_target, embeddings_time_target), dim=1)\n","    veloc = self.fc_veloc_1(veloc)\n","    veloc = self.dropout(veloc)\n","    veloc = self.relu(veloc)\n","    veloc = self.fc_veloc_2(veloc)\n","    return veloc\n","\n","\n","  def forward(self, inputs, targets):\n","    out = self.foward_RNN(inputs)\n","\n","    note_target, duree_target, time_target = targets\n","\n","    note = self.classif_note(out)\n","    duree = self.classif_duree(out, note_target)\n","    time = self.classif_time(out, note_target, duree_target)\n","    veloc = self.reg_veloc(out, note_target, duree_target, time_target)\n","\n","    return note, duree, time, veloc"],"execution_count":40,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"W3yH8Gsu_kA7"},"source":["# **Training**"]},{"cell_type":"markdown","metadata":{"id":"HcOcQvB3fE-f"},"source":["## Accuracy "]},{"cell_type":"code","metadata":{"id":"nBIn4DTjfOnw","executionInfo":{"status":"ok","timestamp":1614550247040,"user_tz":-60,"elapsed":489,"user":{"displayName":"tom vesoul","photoUrl":"","userId":"10135283741654588830"}}},"source":["def validate_model(model, loader):\n","    model.eval()\n","    with torch.no_grad():\n","        correct_note = 0\n","        correct_duree = 0\n","        correct_time = 0\n","        sum_distance = 0\n","\n","        total = 0\n","        count = 0\n","        for batch in loader:\n","            count += 1\n","            batch = torch.reshape(batch, (batch.shape[0], -1 , 4))\n","\n","            notes = batch[:,:,0]\n","            velocity = batch[:,:,1]\n","            duree = batch[:,:,2]\n","            time = batch[:,:,3]\n","\n","            notes_sequence = notes[:,:16].to(device)\n","            notes_target = notes[:,16].to(device)\n","\n","            duree = (duree * 2) // 60 - (duree // 60) - 1\n","            duree = torch.clip(duree, 0, 31)\n","            duree_sequence = duree[:,:16].to(device)\n","            duree_target = duree[:,16].to(device)\n","\n","            time = (time * 2) // 60 - (time // 60)\n","            time = torch.clip(time, 0, 17)\n","            time_sequence = time[:,:16].to(device)\n","            time_target = time[:,16].to(device)\n","\n","            velocity_sequence = velocity[:,:16].to(device) / 100\n","            velocity_sequence = torch.unsqueeze(velocity_sequence, 2)\n","            velocity_target = velocity[:,16].to(device) / 100\n","\n","            inputs = (notes_sequence, duree_sequence, time_sequence, velocity_sequence)\n","            targets = (notes_target, duree_target, time_target)\n","\n","            note, duree, time, veloc = model(inputs, targets)\n","\n","            # Accuracy note\n","            _, predicted = torch.max(note.data, 1)\n","            correct_note += (predicted == notes_target).sum().item()\n","            total += notes_target.size(0)\n","\n","            # Accuracy duree \n","            _, predicted = torch.max(duree.data, 1)\n","            correct_duree += (predicted == duree_target).sum().item()\n","\n","            # Accuracy time\n","            _, predicted = torch.max(time.data, 1)\n","            correct_time += (predicted == time_target).sum().item()\n","\n","            # Distance velocity\n","            distance = torch.mean(torch.abs(veloc.squeeze() - velocity_target)) * 100\n","            sum_distance += distance.item()\n","\n","\n","\n","        accuracy_note = 100 * correct_note / total\n","        accuracy_duree = 100 * correct_duree / total\n","        accuracy_time = 100 * correct_time / total\n","        distance_velo = sum_distance / count\n","\n","    return (accuracy_note, accuracy_duree, accuracy_time, distance_velo)"],"execution_count":41,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0H3umlWRfPNJ"},"source":["## Training loop"]},{"cell_type":"code","metadata":{"id":"ArkqUsBtmsQT","executionInfo":{"status":"ok","timestamp":1614550248614,"user_tz":-60,"elapsed":463,"user":{"displayName":"tom vesoul","photoUrl":"","userId":"10135283741654588830"}}},"source":["def train_model(model, optimizer, train_loader, val_loader, num_epochs, lr_scheduler=None, display_loss=False):\n","  criterion_note = nn.CrossEntropyLoss()\n","  criterion_duree = nn.CrossEntropyLoss()\n","  criterion_time = nn.CrossEntropyLoss()\n","\n","  best_val_accuracy_note = 0\n","  best_epoch_note = 0\n","\n","  best_val_accuracy_duree = 0\n","  best_epoch_duree = 0\n","\n","  best_val_accuracy_time = 0\n","  best_epoch_time = 0\n","\n","  best_val_distance_velo = 1000\n","  best_epoch_velo = 0\n","\n","  for epoch in range(num_epochs):\n","\n","    model.train()\n","\n","    #### UPDATE LEARNING RATE #### \n","    if lr_scheduler == 'multi_steps':\n","        if epoch in [int(num_epochs * 0.5)]:\n","            for param_group in optimizer.param_groups:\n","                param_group['lr'] *= 0.1\n","\n","    for i, batch in enumerate(train_loader):\n","      batch = torch.reshape(batch, (batch.shape[0], -1 , 4))\n","\n","      notes = batch[:,:,0]\n","      velocity = batch[:,:,1]\n","      duree = batch[:,:,2]\n","      time = batch[:,:,3]\n","\n","      notes_sequence = notes[:,:16].to(device)\n","      notes_target = notes[:,16].to(device)\n","\n","      duree = (duree * 2) // 60 - (duree // 60) - 1\n","      duree = torch.clip(duree, 0, 31)\n","      duree_sequence = duree[:,:16].to(device)\n","      duree_target = duree[:,16].to(device)\n","\n","      time = (time * 2) // 60 - (time // 60)\n","      time = torch.clip(time, 0, 17)\n","      time_sequence = time[:,:16].to(device)\n","      time_target = time[:,16].to(device)\n","\n","      velocity_sequence = velocity[:,:16].to(device) / 100\n","      velocity_sequence = torch.unsqueeze(velocity_sequence, 2)\n","      velocity_target = velocity[:,16].to(device) / 100\n","\n","      inputs = (notes_sequence, duree_sequence, time_sequence, velocity_sequence)\n","      targets = (notes_target, duree_target, time_target)\n","\n","      optimizer.zero_grad()\n","      note, duree, time, veloc = model(inputs, targets)\n","\n","      loss_note = torch.mean(criterion_note(note, notes_target))\n","      loss_duree = torch.mean(criterion_duree(duree, duree_target))\n","      loss_time = torch.mean(criterion_time(time, time_target))\n","      loss_veloc = torch.mean(torch.abs(veloc.squeeze() - velocity_target))\n","      loss = loss_note + loss_duree + loss_time + loss_veloc\n","\n","      loss.backward()\n","      optimizer.step()\n","\n","      if i % 300 == 0 and display_loss:\n","        print(f'Epoch : {epoch}, Step: {i}, Loss: {round(loss.item(), 2)}')\n","\n","    # Train accuracy \n","    train_accuracy_note, train_accuracy_duree, train_accuracy_time, train_distance_velo = validate_model(model, train_loader)\n","    train_accuracy_note, train_accuracy_duree, train_accuracy_time, train_distance_velo = round(train_accuracy_note, 2), round(train_accuracy_duree, 2), round(train_accuracy_time, 2), round(train_distance_velo, 2)\n","\n","    # Val accuracy\n","    val_accuracy_note, val_accuracy_duree, val_accuracy_time, val_distance_velo = validate_model(model, val_loader)\n","    val_accuracy_note, val_accuracy_duree, val_accuracy_time, val_distance_velo = round(val_accuracy_note, 2), round(val_accuracy_duree, 2), round(val_accuracy_time, 2), round(val_distance_velo, 2)\n","\n","    if val_accuracy_note > best_val_accuracy_note:\n","      best_val_accuracy_note = val_accuracy_note\n","      best_epoch_note = epoch\n","\n","    if val_accuracy_duree > best_val_accuracy_duree:\n","      best_val_accuracy_duree = val_accuracy_duree\n","      best_epoch_duree = epoch\n","\n","    if val_accuracy_time > best_val_accuracy_time:\n","      best_val_accuracy_time = val_accuracy_time\n","      best_epoch_time = epoch\n","\n","    if val_distance_velo < best_val_distance_velo:\n","      best_val_distance_velo = val_distance_velo\n","      best_epoch_velo = epoch\n","\n","    print('################')\n","    print(f'Epoch: {epoch}, Loss note: {round(loss_note.item(), 2)}, Loss note on: {round(loss_duree.item(), 2)}, Loss time: {round(loss_time.item(), 2)}, Loss velocity: {round(100 * loss_veloc.item(), 2)}')\n","    print('------')\n","    print(f'Epoch : {epoch}, Train accuracy note : {train_accuracy_note} %, Val accuracy note : {val_accuracy_note} %')\n","    print(f'Best val accuracy at epoch {best_epoch_note}: {best_val_accuracy_note} %')\n","    print('------')\n","    print(f'Epoch : {epoch}, Train accuracy duree : {train_accuracy_duree} %, Val accuracy duree : {val_accuracy_duree} %')\n","    print(f'Best val accuracy at epoch {best_epoch_duree}: {best_val_accuracy_duree} %')\n","    print('------')\n","    print(f'Epoch : {epoch}, Train accuracy time : {train_accuracy_time} %, Val accuracy time: {val_accuracy_time} %')\n","    print(f'Best val accuracy at epoch {best_epoch_time}: {best_val_accuracy_time} %')\n","    print('------')\n","    print(f'Epoch : {epoch}, Train distance velo : {train_distance_velo}, Val distance velo: {val_distance_velo}')\n","    print(f'Best val distance at epoch {best_epoch_velo}: {best_val_distance_velo}')"],"execution_count":42,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Y-kI3TzTnA9Z"},"source":["# **Experiments**"]},{"cell_type":"markdown","metadata":{"id":"I_wZ7rx2r5uo"},"source":["## RNN"]},{"cell_type":"code","metadata":{"id":"5rdbT4ZSr48X"},"source":["model = RNN(num_classes_note, num_classes_duree, num_classes_time, embedding_dim_note=128, embedding_dim_duree=128, embedding_dim_time=128, embedding_dim_veloc=128, hidden_size=512, num_layers=3).to(device)\n","optimizer = torch.optim.SGD(model.parameters(), lr=0.05, nesterov=True, momentum=0.9)\n","\n","train_model(model, optimizer, train_loader, val_loader, num_epochs=100, lr_scheduler='multi_steps')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DxRpDkymr7Yu"},"source":["## GRU"]},{"cell_type":"code","metadata":{"id":"OBT8buE0r3WS"},"source":["model = GRU(num_classes_note, num_classes_duree, num_classes_time, embedding_dim_note=128, embedding_dim_duree=128, embedding_dim_time=128, embedding_dim_veloc=128, hidden_size=512, num_layers=3).to(device)\n","optimizer = torch.optim.SGD(model.parameters(), lr=0.05, nesterov=True, momentum=0.9)\n","\n","train_model(model, optimizer, train_loader, val_loader, num_epochs=100, lr_scheduler='multi_steps')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BiFPPpaLTgE6"},"source":["## LSTM"]},{"cell_type":"code","metadata":{"id":"JpTsadAnThoX"},"source":["model = LSTM(num_classes_note, num_classes_duree, num_classes_time, embedding_dim_note=128, embedding_dim_duree=128, embedding_dim_time=128, embedding_dim_veloc=128, hidden_size=512, num_layers=3).to(device)\n","optimizer = torch.optim.SGD(model.parameters(), lr=0.05, nesterov=True, momentum=0.9)\n","\n","train_model(model, optimizer, train_loader, val_loader, num_epochs=100, lr_scheduler='multi_steps')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xJTtHacQdHU3"},"source":["# **Music generation**"]},{"cell_type":"markdown","metadata":{"id":"tPe8tBGFd_WX"},"source":["## Mido utils"]},{"cell_type":"code","metadata":{"id":"li_vB3yOdm6p","executionInfo":{"status":"ok","timestamp":1614550408343,"user_tz":-60,"elapsed":454,"user":{"displayName":"tom vesoul","photoUrl":"","userId":"10135283741654588830"}}},"source":["def data_to_track(data):\n","    track = MidiTrack()\n","    for values in data:\n","        note = values[0]\n","        velocity = values[1]\n","        time = values[2]\n","        track.append(Message('note_on', channel=0, note=note, velocity=velocity, time=time))\n","    return(track)\n","    "],"execution_count":45,"outputs":[]},{"cell_type":"code","metadata":{"id":"7xeZCtS9-kIJ","executionInfo":{"status":"ok","timestamp":1614550408513,"user_tz":-60,"elapsed":431,"user":{"displayName":"tom vesoul","photoUrl":"","userId":"10135283741654588830"}}},"source":["def generate_inverse_track(data_sequence):\n","    track_new = MidiTrack()\n","    \n","    abs_time = 0\n","    messages = []\n","    \n","    for note_vect in data_sequence:\n","        abs_time += note_vect[3]\n","        messages.append([abs_time, note_vect[1], note_vect[0]])\n","        messages.append([abs_time + note_vect[2], 0, note_vect[0]])\n","    \n","    messages.sort()\n","    \n","    time_init = 0\n","    \n","    messages_2 = []\n","    for l in messages:\n","        note = l[2]\n","        velocity = l[1]\n","        time = l[0] - time_init\n","        time_init = l[0]\n","        track_new.append(Message('note_on', channel=0, note=note, velocity=velocity, time=time))\n","    \n","    #mid = MidiFile()\n","    #mid.tracks.append(track_new)\n","    #mid.save(path)\n","    \n","    return track_new"],"execution_count":46,"outputs":[]},{"cell_type":"code","metadata":{"id":"dUh7p2jveSfs","executionInfo":{"status":"ok","timestamp":1614550410057,"user_tz":-60,"elapsed":493,"user":{"displayName":"tom vesoul","photoUrl":"","userId":"10135283741654588830"}}},"source":["def save_track(track, path):\n","    mid = MidiFile()\n","    mid.tracks.append(track)\n","    mid.save(path)"],"execution_count":47,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bxXX-ziatDMh"},"source":["## Generation"]},{"cell_type":"code","metadata":{"id":"-ifERWj2YmOP","executionInfo":{"status":"ok","timestamp":1614550421826,"user_tz":-60,"elapsed":438,"user":{"displayName":"tom vesoul","photoUrl":"","userId":"10135283741654588830"}}},"source":["n_predictions = 1000\n","# Temperature parameters\n","temp_note = 1.5\n","temp_duree = 1.5\n","temp_time = 1.5\n","\n","start_velocity = 50\n","start_note = 64\n","start_duree = 240\n","start_time = 0\n","\n","list_notes = [start_note]\n","list_duree = [start_duree]\n","list_times = [start_time]\n","list_velocity = [start_velocity]\n","\n","data = [[start_note, start_velocity, start_duree, start_time]]"],"execution_count":49,"outputs":[]},{"cell_type":"code","metadata":{"id":"XI81WTmQnO-J","executionInfo":{"status":"ok","timestamp":1614550445406,"user_tz":-60,"elapsed":4208,"user":{"displayName":"tom vesoul","photoUrl":"","userId":"10135283741654588830"}}},"source":["look_back = 16 # take the 16 last notes as input\n","model.eval()\n","\n","for i in range(n_predictions):\n","  list_notes_input = list_notes[-look_back:]\n","  list_duree_input = list_duree[-look_back:]\n","  list_times_input = list_times[-look_back:]\n","  list_velocity_input = list_velocity[-look_back:]\n","\n","  notes_input = torch.reshape(torch.tensor(list_notes_input),(1,-1)).to(device)\n","  duree_input = torch.reshape(torch.tensor(list_duree_input),(1,-1)).to(device)\n","  times_input = torch.reshape(torch.tensor(list_times_input),(1,-1)).to(device)\n","  velo_input = torch.reshape(torch.tensor(list_velocity_input),(1,-1)).to(device)\n","\n","  duree_input = (duree_input * 2) // 60 - (duree_input // 60) - 1\n","  duree_input = torch.clip(duree_input, 0, 31)\n","\n","  times_input = (times_input * 2) // 60 - (times_input // 60)\n","  times_input = torch.clip(times_input, 0, 17)\n","\n","  velo_input = torch.unsqueeze(velo_input, 2) / 100\n","\n","  inputs = (notes_input, duree_input, times_input, velo_input)\n","  out = model.foward_RNN(inputs)\n","\n","  # Sample note\n","  note = model.classif_note(out)\n","  array_proba_note = torch.softmax(note / temp_note, 1).detach().cpu().numpy()[0]\n","  note_sampled = np.random.choice(range(num_classes_note), p=array_proba_note)\n","\n","  note_target = torch.tensor([note_sampled]).to(device)\n","\n","  # Sample duree\n","  duree = model.classif_duree(out, note_target)\n","  array_proba_duree = torch.softmax(duree / temp_duree, 1).detach().cpu().numpy()[0]\n","  duree_sampled = np.random.choice(range(num_classes_duree), p=array_proba_duree)\n","\n","  duree_target = torch.tensor([duree_sampled]).to(device)\n","\n","  # Sample time\n","  time = model.classif_time(out, note_target, duree_target)\n","  array_proba_time = torch.softmax(time / temp_time, 1).detach().cpu().numpy()[0]\n","  time_sampled = np.random.choice(range(num_classes_time), p=array_proba_time)\n","\n","  time_target = torch.tensor([time_sampled]).to(device)\n","\n","  # Compute velocity\n","  velocity = model.reg_veloc(out, note_target, duree_target, time_target)\n","  velocity = torch.clip(100 * velocity, 30, 100)\n","  \n","  final_duree = int((duree_sampled + 1) * 60)\n","  final_time = int(time_sampled * 60)\n","  final_velocity = int(velocity.item())\n","\n","  data.append([note_sampled, final_velocity, final_duree, final_time])\n","\n","  list_notes.append(note_sampled)\n","  list_duree.append(final_duree)\n","  list_times.append(final_time)\n","  list_velocity.append(final_velocity)"],"execution_count":50,"outputs":[]},{"cell_type":"code","metadata":{"id":"8Tbti3brbwPx","executionInfo":{"status":"ok","timestamp":1614550446558,"user_tz":-60,"elapsed":450,"user":{"displayName":"tom vesoul","photoUrl":"","userId":"10135283741654588830"}}},"source":["track = generate_inverse_track(data)\n","save_track(track, 'file.mid')"],"execution_count":51,"outputs":[]}]}