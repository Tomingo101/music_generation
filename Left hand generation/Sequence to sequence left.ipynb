{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Seq to seq final.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyP++nmMLsYj9aUMLECyLNHv"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"gZJznJRC8srI"},"source":["# **Import libraries**"]},{"cell_type":"code","metadata":{"id":"-45eEvdJ8nOr","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1614553560773,"user_tz":-60,"elapsed":2810,"user":{"displayName":"tom vesoul","photoUrl":"","userId":"10135283741654588830"}},"outputId":"cec0466d-49d8-49d9-fafd-a326c82e31c2"},"source":["!pip install mido"],"execution_count":10,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: mido in /usr/local/lib/python3.7/dist-packages (1.2.9)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"cTbDl8PI8fYg","executionInfo":{"status":"ok","timestamp":1614553560773,"user_tz":-60,"elapsed":2650,"user":{"displayName":"tom vesoul","photoUrl":"","userId":"10135283741654588830"}}},"source":["import mido # easy to use python MIDI library\n","import matplotlib.pyplot as plt # plotting\n","import numpy as np # linear algebra\n","import os # accessing directory structure\n","import random\n","import pandas as pd\n","import pickle\n","import time\n","import math\n","\n","from mido import MidiFile, MidiTrack, Message\n","\n","from sklearn import model_selection\n","\n","import torch \n","import torch.nn as nn\n","import torchvision\n","import torchvision.transforms as transforms\n","import torch.nn.functional as F"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C_IWJPBcOPBF","executionInfo":{"status":"ok","timestamp":1614553560773,"user_tz":-60,"elapsed":2498,"user":{"displayName":"tom vesoul","photoUrl":"","userId":"10135283741654588830"}},"outputId":"1fa0ab3c-13b8-4190-c2f6-0fa915ad3e87"},"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","device"],"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["device(type='cuda')"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"markdown","metadata":{"id":"6z1kmYWHNsK4"},"source":["# **Hyperparameters**"]},{"cell_type":"code","metadata":{"id":"H39jJFdVNvUu","executionInfo":{"status":"ok","timestamp":1614553560774,"user_tz":-60,"elapsed":2078,"user":{"displayName":"tom vesoul","photoUrl":"","userId":"10135283741654588830"}}},"source":["num_epochs = 100\n","batch_size = 512\n","\n","sequence_length = 16\n","embedding_dim_note = 128\n","embedding_dim_duree = 128\n","embedding_dim_time = 128\n","embedding_dim_veloc = 128\n","\n","hidden_size = 128\n","num_layers = 3\n","num_classes_note = 128\n","num_classes_duree = 32\n","num_classes_time = 18\n","\n","learning_rate = 0.05"],"execution_count":13,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"L6v_HGOd8y1N"},"source":["# **Load data**"]},{"cell_type":"code","metadata":{"id":"8TFwmJNIWMrM","executionInfo":{"status":"ok","timestamp":1614553560774,"user_tz":-60,"elapsed":1646,"user":{"displayName":"tom vesoul","photoUrl":"","userId":"10135283741654588830"}}},"source":["def retrive_list_of_seq(file_name):\n","    infile = open(file_name,'rb')\n","    list_of_seq = pickle.load(infile)\n","    infile.close()\n","    return list_of_seq"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"id":"KwCxnr9lWbN8","executionInfo":{"status":"ok","timestamp":1614553561625,"user_tz":-60,"elapsed":2282,"user":{"displayName":"tom vesoul","photoUrl":"","userId":"10135283741654588830"}}},"source":["sequences_r = retrive_list_of_seq('sequences_r')\n","sequences_l_p = retrive_list_of_seq('sequences_l_p')\n","sequences_l = retrive_list_of_seq('sequences_l')"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"id":"FtRtt5RPWaxh","executionInfo":{"status":"ok","timestamp":1614553561626,"user_tz":-60,"elapsed":2049,"user":{"displayName":"tom vesoul","photoUrl":"","userId":"10135283741654588830"}}},"source":["def split(list_of_seq_r, list_of_seq_l_p, list_of_seq_l, valset_size = 0.8):\n","    indices = [i for i in range(len(list_of_seq_r))]\n","    random.shuffle(indices)\n","    train_indices, val_indices = indices[:int(valset_size * len(indices))], indices[int(valset_size * len(indices)):]\n","    train_set = [[list_of_seq_r[i] for i in train_indices], [list_of_seq_l_p[i] for i in train_indices], [list_of_seq_l[i] for i in train_indices]]\n","    val_set = [[list_of_seq_r[i] for i in val_indices], [list_of_seq_l_p[i] for i in val_indices], [list_of_seq_l[i] for i in val_indices]]\n","    return train_set, val_set"],"execution_count":16,"outputs":[]},{"cell_type":"code","metadata":{"id":"jaVQ4sLCgxxg","executionInfo":{"status":"ok","timestamp":1614553561626,"user_tz":-60,"elapsed":1171,"user":{"displayName":"tom vesoul","photoUrl":"","userId":"10135283741654588830"}}},"source":["def pad_sequence(seq_batch, start_tag=False):\n","  #seq_batch = seq_all.copy()\n","  max_length = max([len(seq_batch[i]) for i in range(len(seq_batch))])\n","  for i in range(len(seq_batch)):\n","    seq  = seq_batch[i]\n","    n = len(seq)\n","    for k in range(max_length - n):\n","      seq.append([-2, -2, -2, -2])\n","    if start_tag:\n","      seq.insert(0, [-3, -3, -3, -3])\n","  return seq_batch"],"execution_count":17,"outputs":[]},{"cell_type":"code","metadata":{"id":"lObht3wygr1g","executionInfo":{"status":"ok","timestamp":1614553561626,"user_tz":-60,"elapsed":888,"user":{"displayName":"tom vesoul","photoUrl":"","userId":"10135283741654588830"}}},"source":["def generate_batch(dataset, list_id):\n","  seq_r = [dataset[0][index].copy() for index in list_id]\n","  seq_l_p = [dataset[1][index].copy() for index in list_id]\n","  seq_l = [dataset[2][index].copy() for index in list_id]\n","\n","  seq_r_padded = np.array(pad_sequence(seq_r))\n","  seq_l_p_padded = np.array(pad_sequence(seq_l_p))\n","  seq_l_padded = np.array(pad_sequence(seq_l))\n","  \n","  return seq_r_padded, seq_l_p_padded, seq_l_padded"],"execution_count":18,"outputs":[]},{"cell_type":"code","metadata":{"id":"_p3Gqtk8Xhst","executionInfo":{"status":"ok","timestamp":1614553561626,"user_tz":-60,"elapsed":545,"user":{"displayName":"tom vesoul","photoUrl":"","userId":"10135283741654588830"}}},"source":["train_set, val_set = split(sequences_r, sequences_l_p, sequences_l, valset_size = 0.7)\n","n_total_train = len(train_set[0])\n","n_total_val = len(val_set[0])"],"execution_count":19,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5zNjY6fkN_wP"},"source":["# **Models**"]},{"cell_type":"markdown","metadata":{"id":"BNGirM7Kw-uu"},"source":["## Preprocess utils"]},{"cell_type":"code","metadata":{"id":"fJ9Dzxz2xAxY","executionInfo":{"status":"ok","timestamp":1614553563449,"user_tz":-60,"elapsed":544,"user":{"displayName":"tom vesoul","photoUrl":"","userId":"10135283741654588830"}}},"source":["def process_note(x):\n","  if x < 0:\n","    if x == -1:\n","      x = 128\n","    elif x == -2:\n","      x = 129\n","    elif x == -3:\n","      x = 130\n","    return x\n","  else:\n","    return x"],"execution_count":20,"outputs":[]},{"cell_type":"code","metadata":{"id":"v34VYMkCxCy6","executionInfo":{"status":"ok","timestamp":1614553563681,"user_tz":-60,"elapsed":525,"user":{"displayName":"tom vesoul","photoUrl":"","userId":"10135283741654588830"}}},"source":["def process_duree(x):\n","  if x >= 0:\n","    x =  (x * 2) // 60 - (x // 60) - 1\n","    x = min(max(x, 0), 31)\n","    return x\n","  else:\n","    if x == -1:\n","      x = 32\n","    elif x == -2:\n","      x = 33\n","    elif x == -3:\n","      x = 34\n","    return x"],"execution_count":21,"outputs":[]},{"cell_type":"code","metadata":{"id":"bvqZgme_xEmT","executionInfo":{"status":"ok","timestamp":1614553563875,"user_tz":-60,"elapsed":387,"user":{"displayName":"tom vesoul","photoUrl":"","userId":"10135283741654588830"}}},"source":["def process_time(x):\n","  if x >= 0:\n","    x =  (x * 2) // 60 - (x // 60) \n","    x = min(max(x, 0), 17)\n","    return x\n","  else:\n","    if x == -1:\n","      x = 18\n","    elif x == -2:\n","      x = 19\n","    elif x == -3:\n","      x = 20\n","    return x"],"execution_count":22,"outputs":[]},{"cell_type":"code","metadata":{"id":"h7w2EVrBoR7F","executionInfo":{"status":"ok","timestamp":1614553564068,"user_tz":-60,"elapsed":337,"user":{"displayName":"tom vesoul","photoUrl":"","userId":"10135283741654588830"}}},"source":["def process_sequence(seq):\n","  batch = torch.tensor(seq)\n","  batch = torch.reshape(batch, (batch.shape[0], -1 , 4))\n","\n","  note = batch[:,:,0]\n","  velocity = batch[:,:,1]\n","  duree = batch[:,:,2]\n","  time = batch[:,:,3]\n","\n","  note = note.apply_(lambda x :process_note(x))\n","  notes_sequence = note.to(device)\n","\n","  duree = duree.apply_(lambda x :process_duree(x))\n","  duree_sequence = duree.to(device)\n","\n","  time = time.apply_(lambda x :process_time(x))\n","  time_sequence = time.to(device)\n","\n","  velocity_sequence = velocity.to(device) / 100\n","  velocity_sequence = torch.unsqueeze(velocity_sequence, 2)\n","\n","  return (notes_sequence, duree_sequence, time_sequence, velocity_sequence)"],"execution_count":23,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4NdM4AJhsLRe"},"source":["## Encoder main gauche previous"]},{"cell_type":"code","metadata":{"id":"NL28DWxlsOBL","executionInfo":{"status":"ok","timestamp":1614553565112,"user_tz":-60,"elapsed":531,"user":{"displayName":"tom vesoul","photoUrl":"","userId":"10135283741654588830"}}},"source":["# Recurrent neural network (many-to-one)\n","class Encoder_left_p(nn.Module):\n","  def __init__(self, num_classes_note, num_classes_duree, num_classes_time, embedding_dim_note, embedding_dim_duree, embedding_dim_time, embedding_dim_veloc, hidden_size, num_layers, drop_prob=0., drop_fc=0.):\n","    super(Encoder_left_p, self).__init__()\n","\n","    self.embedding_note = nn.Embedding(num_classes_note + 3, embedding_dim_note)\n","    self.embedding_duree = nn.Embedding(num_classes_duree + 3, embedding_dim_duree)\n","    self.embedding_time = nn.Embedding(num_classes_time + 3, embedding_dim_time)\n","    self.fc_emb_veloc = nn.Linear(1, embedding_dim_veloc)\n","\n","    self.hidden_size = hidden_size\n","    self.num_layers = num_layers\n","    self.lstm = nn.LSTM(embedding_dim_note + embedding_dim_duree + embedding_dim_time + embedding_dim_veloc, hidden_size, num_layers, dropout=drop_prob, batch_first=True)\n","    \n","   \n","\n","  def forward(self, inputs, pad_mask):\n","    notes, duree, time, veloc = inputs\n","\n","    # Embedding layer\n","    embeddings_note = self.embedding_note(notes)# Output shape (batch, sequence_length, embedding_dim)\n","    embeddings_duree = self.embedding_duree(duree)\n","    embeddings_time = self.embedding_time(time)\n","    emeddings_veloc = self.fc_emb_veloc(veloc)\n","    #emeddings_veloc =  torch.unsqueeze(emeddings_veloc, 2)\n","    \n","    x = torch.cat((embeddings_note, embeddings_duree, embeddings_time, emeddings_veloc), dim=2)\n","    pad_sequence = torch.nn.utils.rnn.pack_padded_sequence(x, pad_mask, batch_first=True, enforce_sorted=False)\n","\n","    # Set initial hidden and cell states \n","    h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device) \n","    c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n","    \n","    # Forward propagate LSTM\n","    out, hidden = self.lstm(pad_sequence, (h0, c0))  # out: tensor of shape (batch_size, seq_length, hidden_size)\n","\n","    return hidden\n"],"execution_count":24,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wKGjSsTCi4vo"},"source":["## Econder main droite"]},{"cell_type":"code","metadata":{"id":"jyLt9l1zi7aj","executionInfo":{"status":"ok","timestamp":1614553565902,"user_tz":-60,"elapsed":337,"user":{"displayName":"tom vesoul","photoUrl":"","userId":"10135283741654588830"}}},"source":["# Recurrent neural network (many-to-one)\n","class Encoder_right(nn.Module):\n","  def __init__(self, num_classes_note, num_classes_duree, num_classes_time, embedding_dim_note, embedding_dim_duree, embedding_dim_time, embedding_dim_veloc, hidden_size, num_layers, drop_prob=0., drop_fc=0.):\n","    super(Encoder_right, self).__init__()\n","\n","    self.embedding_note = nn.Embedding(num_classes_note + 3, embedding_dim_note)\n","    self.embedding_duree = nn.Embedding(num_classes_duree + 3, embedding_dim_duree)\n","    self.embedding_time = nn.Embedding(num_classes_time + 3, embedding_dim_time)\n","    self.fc_emb_veloc = nn.Linear(1, embedding_dim_veloc)\n","\n","    self.hidden_size = hidden_size\n","    self.num_layers = num_layers\n","    self.lstm = nn.LSTM(embedding_dim_note + embedding_dim_duree + embedding_dim_time + embedding_dim_veloc, hidden_size, num_layers, dropout=drop_prob, batch_first=True)\n","    \n","   \n","\n","  def forward(self, inputs, pad_mask):\n","    notes, duree, time, veloc = inputs\n","\n","    # Embedding layer\n","    embeddings_note = self.embedding_note(notes)# Output shape (batch, sequence_length, embedding_dim)\n","    embeddings_duree = self.embedding_duree(duree)\n","    embeddings_time = self.embedding_time(time)\n","    emeddings_veloc = self.fc_emb_veloc(veloc)\n","    #emeddings_veloc =  torch.unsqueeze(emeddings_veloc, 2)\n","\n","    x = torch.cat((embeddings_note, embeddings_duree, embeddings_time, emeddings_veloc), dim=2)\n","    pad_sequence = torch.nn.utils.rnn.pack_padded_sequence(x, pad_mask, batch_first=True, enforce_sorted=False)\n","\n","    # Set initial hidden and cell states \n","    h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device) \n","    c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n","    \n","    # Forward propagate LSTM\n","    out, hidden = self.lstm(pad_sequence, (h0, c0))  # out: tensor of shape (batch_size, seq_length, hidden_size)\n","\n","    return hidden\n"],"execution_count":25,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_juUAonEwcF6"},"source":["## Decoder"]},{"cell_type":"code","metadata":{"id":"8lKlhKMIwdB-","executionInfo":{"status":"ok","timestamp":1614553615627,"user_tz":-60,"elapsed":585,"user":{"displayName":"tom vesoul","photoUrl":"","userId":"10135283741654588830"}}},"source":["# Recurrent neural network (many-to-one)\n","class Decoder(nn.Module):\n","  def __init__(self, num_classes_note, num_classes_duree, num_classes_time, embedding_dim_note, embedding_dim_duree, embedding_dim_time, embedding_dim_veloc, hidden_size, num_layers, drop_prob=0., drop_fc=0.):\n","    super(Decoder, self).__init__()\n","\n","    self.embedding_note = nn.Embedding(num_classes_note + 3, embedding_dim_note)\n","    self.embedding_duree = nn.Embedding(num_classes_duree + 3, embedding_dim_duree)\n","    self.embedding_time = nn.Embedding(num_classes_time + 3, embedding_dim_time)\n","    self.fc_emb_veloc = nn.Linear(1, embedding_dim_veloc)\n","\n","    self.embedding_note_cond = nn.Embedding(num_classes_note + 3, embedding_dim_note)\n","    self.embedding_duree_cond = nn.Embedding(num_classes_duree + 3, embedding_dim_duree)\n","    self.embedding_time_cond = nn.Embedding(num_classes_time + 3, embedding_dim_time)\n","\n","    self.hidden_size = hidden_size\n","    self.num_layers = num_layers\n","    self.lstm = nn.LSTM(embedding_dim_note + embedding_dim_duree + embedding_dim_time + embedding_dim_veloc, hidden_size, num_layers, dropout=drop_prob, batch_first=True)\n","    \n","    self.fc_note_1 = nn.Linear(hidden_size, hidden_size)\n","    self.fc_note_2 = nn.Linear(hidden_size, num_classes_note + 3)\n","\n","    self.fc_duree_1 = nn.Linear(hidden_size + embedding_dim_note, hidden_size)\n","    self.fc_duree_2 = nn.Linear(hidden_size, num_classes_duree + 3)\n","\n","    self.fc_time_1 = nn.Linear(hidden_size + embedding_dim_note + embedding_dim_duree, hidden_size)\n","    self.fc_time_2 = nn.Linear(hidden_size, num_classes_time + 3)\n","\n","    self.fc_veloc_1 = nn.Linear(hidden_size + embedding_dim_note + embedding_dim_duree + embedding_dim_time, hidden_size)\n","    self.fc_veloc_2 = nn.Linear(hidden_size, 1)\n","\n","\n","    self.relu = nn.ReLU()\n","    self.dropout = nn.Dropout(p=drop_fc)\n","\n","  def forward_RNN(self, inputs, hidden):\n","    notes, duree, time, veloc = inputs\n","\n","    # Embedding layer\n","    embeddings_note = self.embedding_note(notes) # Output shape (batch, sequence_length, embedding_dim)\n","    embeddings_duree = self.embedding_duree(duree)\n","    embeddings_time = self.embedding_time(time)\n","    emeddings_veloc = self.fc_emb_veloc(veloc)\n","    #emeddings_veloc =  torch.unsqueeze(emeddings_veloc, 2)\n","\n","    x = torch.cat((embeddings_note, embeddings_duree, embeddings_time, emeddings_veloc), dim=2)\n","    \n","    # Forward propagate LSTM\n","    out, hidden = self.lstm(x, hidden)  # out: tensor of shape (batch_size, seq_length, hidden_size)\n","    out = out[:, -1, :] # Hidden state of the last element of the sequence (equivalent to hidden[0])\n","\n","    return out, hidden\n","\n","  def classif_note(self, out):\n","    note = self.fc_note_1(out)\n","    note = self.dropout(note)\n","    note = self.relu(note)\n","    note = self.fc_note_2(note)\n","    return note\n","\n","  def classif_duree(self, out, note_target):\n","\n","    embeddings_note_target = self.embedding_note_cond(note_target)\n","\n","    duree = torch.cat((out, embeddings_note_target), dim=1)\n","    duree = self.fc_duree_1(duree)\n","    duree = self.dropout(duree)\n","    duree = self.relu(duree)\n","    duree = self.fc_duree_2(duree)\n","    return duree\n","\n","  def classif_time(self, out, note_target, duree_target):\n","\n","    embeddings_note_target = self.embedding_note_cond(note_target)\n","    embeddings_duree_target = self.embedding_duree_cond(duree_target)\n","\n","    time = torch.cat((out, embeddings_note_target, embeddings_duree_target), dim=1)\n","    time = self.fc_time_1(time)\n","    time = self.dropout(time)\n","    time = self.relu(time)\n","    time = self.fc_time_2(time)\n","    return time\n","\n","  def reg_veloc(self, out, note_target, duree_target, time_target):\n","\n","    embeddings_note_target = self.embedding_note_cond(note_target)\n","    embeddings_duree_target = self.embedding_duree_cond(duree_target)\n","    embeddings_time_target = self.embedding_time_cond(time_target)\n","\n","    veloc = torch.cat((out, embeddings_note_target, embeddings_duree_target, embeddings_time_target), dim=1)\n","    veloc = self.fc_veloc_1(veloc)\n","    veloc = self.dropout(veloc)\n","    veloc = self.relu(veloc)\n","    veloc = self.fc_veloc_2(veloc)\n","    return veloc\n","\n","\n","  def forward(self, inputs, targets, hidden):\n","    out, hidden = self.forward_RNN(inputs, hidden)\n","\n","    note_target, duree_target, time_target = targets\n","\n","    note = self.classif_note(out)\n","    duree = self.classif_duree(out, note_target)\n","    time = self.classif_time(out, note_target, duree_target)\n","    veloc = self.reg_veloc(out, note_target, duree_target, time_target)\n","\n","    return note, duree, time, veloc, hidden"],"execution_count":29,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"W3yH8Gsu_kA7"},"source":["# **Training**"]},{"cell_type":"markdown","metadata":{"id":"HcOcQvB3fE-f"},"source":["## Accuracy "]},{"cell_type":"code","metadata":{"id":"iAxkPwCUM43H","executionInfo":{"status":"ok","timestamp":1614553617576,"user_tz":-60,"elapsed":726,"user":{"displayName":"tom vesoul","photoUrl":"","userId":"10135283741654588830"}}},"source":["def validate_model(models, dataset, use_teacher_forcing = True, max_iter=100000):\n","\n","    n_total_set = len(dataset[0])\n","    encoder_right, encoder_left_p, decoder_left = models\n","    encoder_right.eval()\n","    encoder_left_p.eval()\n","    decoder_left.eval()\n","\n","    with torch.no_grad():\n","        correct_note = 0\n","        correct_duree = 0\n","        correct_time = 0\n","        sum_distance = 0\n","\n","        total = 0\n","        count = 0\n","        # Batch generation\n","        set_id_epoch = set(range(n_total_set))\n","        n_iter_per_epoch = int(len(set_id_epoch) / batch_size) + 1\n","\n","        for iter in range(n_iter_per_epoch):\n","          if iter >= max_iter:\n","            break\n","\n","          list_id_batch = random.sample(set_id_epoch, min(batch_size, len(set_id_epoch)))\n","          for id in list_id_batch: # Remove id from set\n","            set_id_epoch.remove(id)\n","\n","          current_batch_size = len(list_id_batch)\n","          \n","\n","          ################# PROCESS SEQUENCE #################\n","\n","          seq_r, seq_l_p, seq_l  = generate_batch(dataset, list_id_batch)\n","\n","          (notes_sequence_r, duree_sequence_r, time_sequence_r, velocity_sequence_r) = process_sequence(seq_r)\n","          (notes_sequence_l_p, duree_sequence_l_p, time_sequence_l_p, velocity_sequence_l_p) = process_sequence(seq_l_p)\n","          (notes_sequence_l, duree_sequence_l, time_sequence_l, velocity_sequence_l) = process_sequence(seq_l)\n","\n","          ################### ENCODING ###################\n","\n","          # Encode right\n","          mask_r = 1 - (notes_sequence_r == 129).float()\n","          pad_mask_r = torch.sum(mask_r, 1).int().cpu().detach().numpy()\n","\n","          inputs_r = (notes_sequence_r, duree_sequence_r, time_sequence_r, velocity_sequence_r)\n","          hidden_right = encoder_right(inputs_r, pad_mask_r)\n","          \n","          # Encode left previous\n","          mask_l_p = 1 - (notes_sequence_l_p == 129).float()\n","          pad_mask_l_p = torch.sum(mask_l_p, 1).int().cpu().detach().numpy()\n","\n","          inputs_l_p = (notes_sequence_l_p, duree_sequence_l_p, time_sequence_l_p, velocity_sequence_l_p)\n","          hidden_left_p = encoder_left_p(inputs_l_p, pad_mask_l_p)\n","\n","          # Concatenate hidden states\n","          hidden_1_r, hidden_2_r = hidden_right\n","          hidden_1_lp, hidden_2_lp = hidden_left_p\n","\n","          hidden_1 = torch.cat((hidden_1_r, hidden_1_lp), dim=2)\n","          hidden_2 = torch.cat((hidden_2_r, hidden_2_lp), dim=2)\n","\n","          #hidden_1 = torch.randn((hidden_1.shape[0], hidden_1.shape[1], hidden_1.shape[2])).to(device)\n","          #hidden_2 = torch.randn((hidden_2.shape[0], hidden_2.shape[1], hidden_2.shape[2])).to(device)\n","\n","          hidden = (hidden_1, hidden_2)\n","\n","          ################### DECODING ###################\n","\n","          # Decoder initial input (start tag)\n","          stack_decoder_input = torch.unsqueeze(torch.tensor([[130, 34, 20, -1]], device=device).repeat(current_batch_size, 1), 1).to(device)\n","\n","          note_input = stack_decoder_input[:,:,0]\n","          duree_input = stack_decoder_input[:,:,1]\n","          time_input = stack_decoder_input[:,:,2]\n","          velocity_input = stack_decoder_input[:,:,3].float()\n","          velocity_input = torch.unsqueeze(velocity_input, 2)\n","\n","          decoder_inputs = (note_input, duree_input, time_input, velocity_input)\n","\n","          # Teacher forcing\n","          teacher_forcing_ratio = 0.5\n","          use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n","          use_teacher_forcing\n","\n","          # Decode left sequence\n","          length_batch = notes_sequence_l.shape[1]\n","\n","          loss_total_note = 0\n","          loss_total_duree = 0\n","          loss_total_time = 0\n","          loss_total_veloc = 0\n","\n","          loss_total = torch.tensor(0., device=device)\n","\n","          for i in range(length_batch):\n","            count += 1\n","            # Predictions\n","            note_target = notes_sequence_l[:,i]\n","            duree_target = duree_sequence_l[:,i]\n","            time_target = time_sequence_l[:,i]\n","            velocity_target = velocity_sequence_l[:,i]\n","\n","            targets = (note_target, duree_target, time_target)\n","            note_pred_logits, duree_pred_logits, time_pred_logits, veloc_pred, hidden = decoder(decoder_inputs, targets,  hidden)\n","\n","            mask = 1 - (note_target ==  129).float()\n","            count_non_null = torch.sum(mask)\n","\n","            # Accuracy note\n","            _, predicted = torch.max(note_pred_logits.data, 1)\n","            #print('----')\n","            #print(predicted.shape)\n","            #print(note_target.shape)\n","            #print(predicted[:10])\n","            #print(note_target[:10])\n","            correct_note += (predicted * mask == note_target).sum().item()\n","            total += count_non_null.item()\n","\n","            # Accuracy duree \n","            _, predicted = torch.max(duree_pred_logits.data, 1)\n","            correct_duree += (predicted * mask == duree_target).sum().item()\n","            #print(predicted[:10])\n","            #print(duree_target[:10])\n","\n","            # Accuracy time\n","            _, predicted = torch.max(time_pred_logits.data, 1)\n","            correct_time += (predicted * mask == time_target).sum().item()\n","            #print(predicted[:10])\n","            #print(time_target[:10])\n","\n","            # Distance velocity\n","            #distance = torch.mean(torch.abs(veloc_pred - velocity_target)) * 100\n","            distance = 100 * torch.sum(mask * torch.abs(veloc_pred.squeeze() - velocity_target.squeeze())) / count_non_null.item()\n","            sum_distance += distance.item()\n","            #print(veloc_pred[:20])\n","            #print(velocity_target[:20])\n","\n","          # New inputs\n","          if use_teacher_forcing:\n","            note_input = torch.unsqueeze(note_target, 1)\n","            duree_input = torch.unsqueeze(duree_target, 1)\n","            time_input = torch.unsqueeze(time_target, 1)\n","            velocity_input = torch.unsqueeze(velocity_target, 1)\n","\n","          else:\n","            _, note_pred = torch.max(note_pred_logits.data, 1)\n","            _, duree_pred = torch.max(duree_pred_logits.data, 1)\n","            _, time_pred = torch.max(time_pred_logits.data, 1)\n","\n","            note_input = torch.unsqueeze(note_pred, 1)\n","            duree_input = torch.unsqueeze(duree_pred, 1)\n","            time_input = torch.unsqueeze(time_pred, 1)\n","            velocity_input = torch.unsqueeze(veloc_pred, 1)\n","\n","          decoder_inputs = (note_input.detach(), duree_input.detach(), time_input.detach(), velocity_input.detach())            \n","\n","\n","        accuracy_note = 100 * correct_note / total\n","        accuracy_duree = 100 * correct_duree / total\n","        accuracy_time = 100 * correct_time / total\n","        distance_velo = sum_distance / count\n","\n","    return (accuracy_note, accuracy_duree, accuracy_time, distance_velo)"],"execution_count":30,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0H3umlWRfPNJ"},"source":["# **Training loop** "]},{"cell_type":"code","metadata":{"id":"LtEboWQUjMDS","executionInfo":{"status":"ok","timestamp":1614553618556,"user_tz":-60,"elapsed":1116,"user":{"displayName":"tom vesoul","photoUrl":"","userId":"10135283741654588830"}}},"source":["def train_model(models, optimizers, train_set, val_set, num_epochs, lr_scheduler=None, display_loss=False, teacher_forcing_ratio = 1., max_train_iter = 100000000):\n","  encoder_right, encoder_left_p, decoder_left = models\n","  optimizer_right, optimizer_left_p, optimizer_left = optimizers\n","\n","  criterion_note = nn.CrossEntropyLoss(reduce=False)\n","  criterion_duree = nn.CrossEntropyLoss(reduce=False)\n","  criterion_time = nn.CrossEntropyLoss(reduce=False)\n","\n","  best_val_accuracy_note = 0\n","  best_epoch_note = 0\n","\n","  best_val_accuracy_duree = 0\n","  best_epoch_duree = 0\n","\n","  best_val_accuracy_time = 0\n","  best_epoch_time = 0\n","\n","  best_val_distance_velo = 1000\n","  best_epoch_velo = 0\n","\n","  n_total_train = len(train_set[0])\n","  n_total_val = len(val_set[0]) \n","\n","  for epoch in range(num_epochs):\n","\n","    start = time.time()\n","\n","    encoder_right.train()\n","    encoder_left_p.train()\n","    decoder_left.train()\n","\n","    #### UPDATE LEARNING RATE #### \n","    if lr_scheduler == 'multi_steps':\n","        if epoch in [int(num_epochs * 0.5)]:\n","            for param_group in optimizer_right.param_groups:\n","                param_group['lr'] *= 0.1\n","            for param_group in optimizer_left_p.param_groups:\n","                param_group['lr'] *= 0.1\n","            for param_group in optimizer_left.param_groups:\n","                param_group['lr'] *= 0.1\n","\n","\n","    # Batch generation\n","    set_id_epoch = set(range(n_total_train))\n","    n_iter_per_epoch = int(len(set_id_epoch) / batch_size) + 1\n","\n","    #for iter in range(n_iter_per_epoch):\n","    for iter in range(min(max_train_iter, n_iter_per_epoch)):\n","      list_id_batch = random.sample(set_id_epoch, min(batch_size, len(set_id_epoch)))\n","      for id in list_id_batch: # Remove id from set\n","        set_id_epoch.remove(id)\n","\n","      current_batch_size = len(list_id_batch)\n","\n","\n","      ################# PROCESS SEQUENCE #################\n","\n","      seq_r, seq_l_p, seq_l  = generate_batch(train_set, list_id_batch)\n","\n","      (notes_sequence_r, duree_sequence_r, time_sequence_r, velocity_sequence_r) = process_sequence(seq_r)\n","      (notes_sequence_l_p, duree_sequence_l_p, time_sequence_l_p, velocity_sequence_l_p) = process_sequence(seq_l_p)\n","      (notes_sequence_l, duree_sequence_l, time_sequence_l, velocity_sequence_l) = process_sequence(seq_l)\n","\n","      ################### ENCODING ###################\n","\n","      # Encode right\n","      mask_r = 1 - (notes_sequence_r == 129).float()\n","      pad_mask_r = torch.sum(mask_r, 1).int().cpu().detach().numpy()\n","\n","      inputs_r = (notes_sequence_r, duree_sequence_r, time_sequence_r, velocity_sequence_r)\n","      hidden_right = encoder_right(inputs_r, pad_mask_r)\n","      \n","      # Encode left previous\n","      mask_l_p = 1 - (notes_sequence_l_p == 129).float()\n","      pad_mask_l_p = torch.sum(mask_l_p, 1).int().cpu().detach().numpy()\n","\n","      inputs_l_p = (notes_sequence_l_p, duree_sequence_l_p, time_sequence_l_p, velocity_sequence_l_p)\n","      hidden_left_p = encoder_left_p(inputs_l_p, pad_mask_l_p)\n","\n","      # Concatenate hidden states\n","      hidden_1_r, hidden_2_r = hidden_right\n","      hidden_1_lp, hidden_2_lp = hidden_left_p\n","\n","      hidden_1 = torch.cat((hidden_1_r, hidden_1_lp), dim=2)\n","      hidden_2 = torch.cat((hidden_2_r, hidden_2_lp), dim=2)\n","\n","      #hidden_1 = torch.randn((hidden_1.shape[0], hidden_1.shape[1], hidden_1.shape[2])).to(device)\n","      #hidden_2 = torch.randn((hidden_2.shape[0], hidden_2.shape[1], hidden_2.shape[2])).to(device)\n","\n","      hidden = (hidden_1, hidden_2)\n","\n","      ################### DECODING ###################\n","\n","      # Decoder initial input (start tag)\n","      stack_decoder_input = torch.unsqueeze(torch.tensor([[130, 34, 20, -1]], device=device).repeat(current_batch_size, 1), 1).to(device)\n","\n","      note_input = stack_decoder_input[:,:,0]\n","      duree_input = stack_decoder_input[:,:,1]\n","      time_input = stack_decoder_input[:,:,2]\n","      velocity_input = stack_decoder_input[:,:,3].float()\n","      velocity_input = torch.unsqueeze(velocity_input, 2)\n","\n","      decoder_inputs = (note_input, duree_input, time_input, velocity_input)\n","\n","      # Teacher forcing\n","      use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n","      use_teacher_forcing\n","\n","      # Decode left sequence\n","      \n","      loss_total = torch.tensor(0., device=device)\n","      length_batch = notes_sequence_l.shape[1]\n","      total_count_batch = 0\n","\n","      for i in range(length_batch):\n","        # Predictions\n","        note_target = notes_sequence_l[:,i]\n","        duree_target = duree_sequence_l[:,i]\n","        time_target = time_sequence_l[:,i]\n","        velocity_target = velocity_sequence_l[:,i]\n","\n","        targets = (note_target, duree_target, time_target)\n","        note_pred_logits, duree_pred_logits, time_pred_logits, veloc_pred, hidden = decoder(decoder_inputs, targets,  hidden)\n","\n","        # Compute loss\n","        mask = 1 - (note_target ==  129).float()\n","        count_non_null = torch.sum(mask)\n","        total_count_batch += count_non_null.item()\n","        #print('----')\n","        #print(mask.shape)\n","        #print(veloc_pred.shape)\n","        #print(velocity_target.shape)\n","        #print(torch.abs(veloc_pred - velocity_target).shape)\n","        #print((mask * torch.abs(veloc_pred - velocity_target)).shape)\n","        \n","\n","        loss_note = torch.sum(mask * criterion_note(note_pred_logits, note_target))\n","        loss_duree = torch.sum(mask * criterion_duree(duree_pred_logits, duree_target)) \n","        loss_time = torch.sum(mask * criterion_time(time_pred_logits, time_target)) \n","        loss_veloc = torch.sum(mask * torch.abs(veloc_pred.squeeze() - velocity_target.squeeze()))\n","\n","\n","        #print((loss_note + loss_duree + loss_time + loss_veloc))\n","        loss_total += ( loss_note + loss_duree + loss_time + loss_veloc)\n","\n","        # New inputs\n","        if use_teacher_forcing:\n","          note_input = torch.unsqueeze(note_target, 1)\n","          duree_input = torch.unsqueeze(duree_target, 1)\n","          time_input = torch.unsqueeze(time_target, 1)\n","          velocity_input = torch.unsqueeze(velocity_target, 1)\n","\n","        else:\n","          _, note_pred = torch.max(note_pred_logits.data, 1)\n","          _, duree_pred = torch.max(duree_pred_logits.data, 1)\n","          _, time_pred = torch.max(time_pred_logits.data, 1)\n","\n","          note_input = torch.unsqueeze(note_pred, 1)\n","          duree_input = torch.unsqueeze(duree_pred, 1)\n","          time_input = torch.unsqueeze(time_pred, 1)\n","          velocity_input = torch.unsqueeze(veloc_pred, 1)\n","\n","        #decoder_inputs = (note_input.detach(), duree_input.detach(), time_input.detach(), velocity_input.detach())\n","        decoder_inputs = (note_input, duree_input, time_input, velocity_input)\n","\n","        if i == 0:\n","          loss_note_display = loss_note.item() / count_non_null.item()\n","          loss_duree_display = loss_duree.item() / count_non_null.item()\n","          loss_time_display = loss_time.item() / count_non_null.item()\n","          loss_veloc_display = loss_veloc.item() / count_non_null.item()\n","\n","      #print(criterion_time(time_pred_logits, time_target))\n","      #print(criterion_note(note_pred_logits, note_target))\n","      #print(mask)\n","  \n","\n","      # Backward and weight update\n","      optimizer_right.zero_grad()\n","      optimizer_left_p.zero_grad()\n","      optimizer_left.zero_grad()\n","\n","      loss_total = loss_total / total_count_batch\n","      loss_total.backward()\n","\n","      optimizer_right.step()\n","      optimizer_left_p.step()\n","      optimizer_left.step()\n","    \n","    ################### VALIDATION ###################\n","    # Train accuracy \n","    train_accuracy_note, train_accuracy_duree, train_accuracy_time, train_distance_velo = validate_model(models, train_set, use_teacher_forcing = True, max_iter=4)\n","    train_accuracy_note, train_accuracy_duree, train_accuracy_time, train_distance_velo = round(train_accuracy_note, 2), round(train_accuracy_duree, 2), round(train_accuracy_time, 2), round(train_distance_velo, 2)\n","\n","    # Val accuracy\n","    val_accuracy_note, val_accuracy_duree, val_accuracy_time, val_distance_velo = validate_model(models, val_set, use_teacher_forcing = True, max_iter=4)\n","    val_accuracy_note, val_accuracy_duree, val_accuracy_time, val_distance_velo = round(val_accuracy_note, 2), round(val_accuracy_duree, 2), round(val_accuracy_time, 2), round(val_distance_velo, 2)\n","\n","    if val_accuracy_note > best_val_accuracy_note:\n","      best_val_accuracy_note = val_accuracy_note\n","      best_epoch_note = epoch\n","\n","    if val_accuracy_duree > best_val_accuracy_duree:\n","      best_val_accuracy_duree = val_accuracy_duree\n","      best_epoch_duree = epoch\n","\n","    if val_accuracy_time > best_val_accuracy_time:\n","      best_val_accuracy_time = val_accuracy_time\n","      best_epoch_time = epoch\n","\n","    if val_distance_velo < best_val_distance_velo:\n","      best_val_distance_velo = val_distance_velo\n","      best_epoch_velo = epoch\n","\n","    end = time.time()\n","\n","    print('################')\n","    print(f'Epoch: {epoch}, Time: {round(end - start, 2)}, Loss note: {round(loss_note_display, 4)}, Loss  duree: {round(loss_duree_display, 4)}, Loss time: {round(loss_time_display, 4)}, Loss velocity: {round(100 * loss_veloc_display, 2)}, Length batch:Â {length_batch}')\n","    print('------')\n","    print(f'Epoch : {epoch}, Train accuracy note : {train_accuracy_note} %, Val accuracy note : {val_accuracy_note} %')\n","    print(f'Best val accuracy at epoch {best_epoch_note}: {best_val_accuracy_note} %')\n","    print('------')\n","    print(f'Epoch : {epoch}, Train accuracy duree : {train_accuracy_duree} %, Val accuracy duree : {val_accuracy_duree} %')\n","    print(f'Best val accuracy at epoch {best_epoch_duree}: {best_val_accuracy_duree} %')\n","    print('------')\n","    print(f'Epoch : {epoch}, Train accuracy time : {train_accuracy_time} %, Val accuracy time: {val_accuracy_time} %')\n","    print(f'Best val accuracy at epoch {best_epoch_time}: {best_val_accuracy_time} %')\n","    print('------')\n","    print(f'Epoch : {epoch}, Train distance velo : {train_distance_velo}, Val distance velo: {val_distance_velo}')\n","    print(f'Best val distance at epoch {best_epoch_velo}: {best_val_distance_velo}')\n","    #print('################')"],"execution_count":31,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"p8B7LMyoVK2c"},"source":["# **Experiments**"]},{"cell_type":"code","metadata":{"id":"IfxS8E8Sn3Zg","executionInfo":{"status":"ok","timestamp":1614553619239,"user_tz":-60,"elapsed":700,"user":{"displayName":"tom vesoul","photoUrl":"","userId":"10135283741654588830"}}},"source":["encoder_right = Encoder_right(num_classes_note, num_classes_duree, num_classes_time, embedding_dim_note=128, embedding_dim_duree=128, embedding_dim_time=128, embedding_dim_veloc=128, hidden_size=256, num_layers=3).to(device)\n","encoder_left_p = Encoder_left_p(num_classes_note, num_classes_duree, num_classes_time, embedding_dim_note=128, embedding_dim_duree=128, embedding_dim_time=128, embedding_dim_veloc=128, hidden_size=256, num_layers=3).to(device)\n","decoder = Decoder(num_classes_note, num_classes_duree, num_classes_time, embedding_dim_note=128, embedding_dim_duree=128, embedding_dim_time=128, embedding_dim_veloc=128, hidden_size=512, num_layers=3).to(device)\n","\n","start_lr = 0.05\n","\n","optimizer_right = torch.optim.SGD(encoder_right.parameters(), lr=start_lr, nesterov=True, momentum=0.9)\n","optimizer_left_p = torch.optim.SGD(encoder_left_p.parameters(), lr=start_lr, nesterov=True, momentum=0.9)\n","optimizer_left = torch.optim.SGD(decoder.parameters(), lr=start_lr, nesterov=True, momentum=0.9)\n","\n","models = (encoder_right, encoder_left_p, decoder)\n","optimizers = (optimizer_right, optimizer_left_p, optimizer_left)\n"],"execution_count":32,"outputs":[]},{"cell_type":"code","metadata":{"id":"xRxKhqRjfFFY"},"source":["train_model(models=models, optimizers=optimizers, train_set=train_set, val_set=val_set, num_epochs=200, lr_scheduler=None, display_loss=False, teacher_forcing_ratio=1., max_train_iter=1000000000)"],"execution_count":null,"outputs":[]}]}